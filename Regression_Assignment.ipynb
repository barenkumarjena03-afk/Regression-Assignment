{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression Assignment**\n",
        "---"
      ],
      "metadata": {
        "id": "VIpbhUyklOsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical and machine learning technique used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the data.\n",
        "\n",
        "It assumes that the relationship between X and Y is linear and is represented by the equation:\n",
        "\n",
        "Y = β₀ + β₁X + ε\n",
        "\n",
        "Where:\n",
        "\n",
        "Y → Dependent (target) variable\n",
        "\n",
        "X → Independent (input) variable\n",
        "\n",
        "β₀ → Intercept (value of Y when X = 0)\n",
        "\n",
        "β₁ → Slope (change in Y for a one-unit change in X)\n",
        "\n",
        "ε → Error term (difference between actual and predicted values)\n"
      ],
      "metadata": {
        "id": "LtYYotcylVNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "a. Linearity\n",
        "\n",
        "There is a linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "b. Independence\n",
        "\n",
        "The observations (errors) are independent of each other.\n",
        "\n",
        "c. Homoscedasticity\n",
        "\n",
        "The variance of the errors is constant across all values of X.\n",
        "\n",
        "d. Normality of Errors\n",
        "\n",
        "The residuals (errors) are normally distributed, especially important for hypothesis testing.\n",
        "\n",
        "e. No Multicollinearity\n",
        "\n",
        "Since it is simple linear regression, there is only one independent variable, so multicollinearity is not present.\n",
        "\n",
        "f. Zero Mean of Errors\n",
        "\n",
        "The expected value of the error term is zero."
      ],
      "metadata": {
        "id": "TcHp2IV0mCCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "In the equation Y = mX + c, the coefficient m represents the slope of the line.\n",
        "\n",
        "Meaning of m:\n",
        "\n",
        "-It shows the change in Y for a one-unit increase in X.\n",
        "\n",
        "-It indicates the strength and direction of the relationship between X and Y.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "-m > 0 → Y increases as X increases (positive relationship)\n",
        "\n",
        "-m < 0 → Y decreases as X increases (negative relationship)\n",
        "\n",
        "-m = 0 → No relationship between X and Y"
      ],
      "metadata": {
        "id": "pPHTAbTpmhQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "In the equation Y = mX + c, the intercept 'c' represents the value of Y when X is equal to 0.\n",
        "\n",
        "Meaning of c:\n",
        "\n",
        "* It is the point where the regression line intersects the Y-axis.\n",
        "* It indicates the baseline or starting value of the dependent variable before any influence of X.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* If c > 0, Y has a positive value when X = 0\n",
        "* If c < 0, Y has a negative value when X = 0\n",
        "* If c = 0, the line passes through the origin bold"
      ],
      "metadata": {
        "id": "aZvV1mxJnc_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "How the slope (m) is calculated in Simple Linear Regression:\n",
        "\n",
        "-First, find the average (mean) of the X values.\n",
        "\n",
        "-Then, find the average (mean) of the Y values.\n",
        "\n",
        "-For each data point:\n",
        "\n",
        ">Subtract the mean of X from the X value.\n",
        "\n",
        ">Subtract the mean of Y from the Y value.\n",
        "\n",
        ">Multiply these two differences.\n",
        "\n",
        "-Add all those multiplied values together.\n",
        "\n",
        "-Separately, for each X value:\n",
        "\n",
        ">Subtract the mean of X from the X value.\n",
        "\n",
        ">Square this difference.\n",
        "\n",
        "-Add all the squared values.\n",
        "\n",
        "-Finally, divide:\n",
        "\n",
        ">(Sum of multiplied differences of X and Y)\n",
        "\n",
        ">by\n",
        "\n",
        ">(Sum of squared differences of X)\n",
        "\n",
        "The result of this division gives the slope (m)."
      ],
      "metadata": {
        "id": "7Bg50hrupGC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The purpose of the Least Squares Method in Simple Linear Regression is to find the best-fitting straight line for the given data.\n",
        "\n",
        "It works by:\n",
        "\n",
        ">Calculating the difference between the actual values and the predicted values of Y for each data point\n",
        "\n",
        ">Squaring these differences (errors)\n",
        "\n",
        ">Adding all the squared errors together\n",
        "\n",
        "The regression line is chosen so that this total squared error is as small as possible."
      ],
      "metadata": {
        "id": "XrgMBKJxqFVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "The coefficient of determination (R²) in Simple Linear Regression measures how well the independent variable (X) explains the variation in the dependent variable (Y).\n",
        "\n",
        "Interpretation of R²:\n",
        "\n",
        ">It represents the proportion of variance in Y that is explained by X.\n",
        "\n",
        ">Its value ranges from 0 to 1.\n",
        "\n",
        "Meaning of values:\n",
        "\n",
        ">R² = 0 → X does not explain any variation in Y\n",
        "\n",
        ">R² = 1 → X explains all the variation in Y\n",
        "\n",
        ">Higher R² value → Better model fit\n",
        "\n",
        "Example:\n",
        "If R² = 0.75, it means 75% of the variation in Y is explained by X, and the remaining 25% is due to other factors or noise."
      ],
      "metadata": {
        "id": "pGVCurWSqVSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression is a statistical and machine learning technique used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xₙ).\n",
        "\n",
        "It extends simple linear regression by using multiple input features to make predictions.\n",
        "\n",
        "Key points:\n",
        "\n",
        "-It shows how multiple factors together affect a single outcome.\n",
        "\n",
        "-Each independent variable has its own coefficient, representing its individual impact on Y while keeping other variables constant.\n",
        "\n",
        "Example:\n",
        "Predicting house price (Y) using house size, number of rooms, and location score (X variables)."
      ],
      "metadata": {
        "id": "d6g0NmaRqme9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "The main difference between Simple Linear Regression and Multiple Linear Regression is the number of independent variables used.\n",
        "\n",
        ">Simple Linear Regression uses only one independent variable to predict the dependent variable.\n",
        "\n",
        ">Multiple Linear Regression uses two or more independent variables to predict the dependent variable."
      ],
      "metadata": {
        "id": "CMVwEGFguiHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "a. Linearity\n",
        "\n",
        "The relationship between each independent variable and the dependent variable is linear.\n",
        "\n",
        "b. Independence of Errors\n",
        "\n",
        "The residuals (errors) are independent of each other.\n",
        "\n",
        "c. Homoscedasticity\n",
        "\n",
        "The variance of the residuals is constant across all levels of the independent variables.\n",
        "\n",
        "d. Normality of Errors\n",
        "\n",
        "The residuals are normally distributed, especially important for inference.\n",
        "\n",
        "e. No Multicollinearity\n",
        "\n",
        "Independent variables should not be highly correlated with each other.\n",
        "\n",
        "f. No Autocorrelation\n",
        "\n",
        "Errors should not be correlated across observations (important in time-series data).\n",
        "\n",
        "g. Correct Model Specification\n",
        "\n",
        "The model includes all relevant variables and excludes irrelevant ones."
      ],
      "metadata": {
        "id": "iMxdOKYjuujT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity occurs in Multiple Linear Regression when the variance of the error terms (residuals) is not constant across all levels of the independent variables.\n",
        "\n",
        "What it means:\n",
        "\n",
        "-The spread of residuals increases or decreases as the predicted values change.\n",
        "\n",
        "-Errors are unevenly distributed instead of having equal variance.\n",
        "\n",
        "How it affects the model:\n",
        "\n",
        "-Regression coefficients remain unbiased, but\n",
        "\n",
        "-Standard errors become unreliable.\n",
        "\n",
        "-This leads to incorrect t-tests, F-tests, and confidence intervals.\n",
        "\n",
        "-As a result, we may draw wrong conclusions about variable significance."
      ],
      "metadata": {
        "id": "NFNVRfbUvq3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "To improve a Multiple Linear Regression model with high multicollinearity, you can use the following methods:\n",
        "\n",
        "a. Remove highly correlated variables\n",
        "\n",
        "Drop one of the independent variables that are strongly correlated with each other.\n",
        "\n",
        "b. Combine correlated features\n",
        "\n",
        "Create a single feature using methods like feature averaging or Principal Component Analysis (PCA).\n",
        "\n",
        "c. Use Regularization techniques\n",
        "\n",
        "Apply Ridge Regression or Lasso Regression, which reduce the impact of multicollinearity by penalizing large coefficients.\n",
        "\n",
        "d. Check Variance Inflation Factor (VIF)\n",
        "\n",
        "Identify and remove variables with very high VIF values.\n",
        "\n",
        "e. Collect more data\n",
        "\n",
        "Increasing sample size can sometimes reduce multicollinearity effects."
      ],
      "metadata": {
        "id": "pRove5YPwvH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Some common techniques for transforming categorical variables for use in regression models are:\n",
        "\n",
        "a. Label Encoding\n",
        "\n",
        "Assigns a unique numerical value to each category.\n",
        "\n",
        "Useful for ordinal data where order matters.\n",
        "\n",
        "b.One-Hot Encoding (Dummy Variables)\n",
        "\n",
        "Creates separate binary columns for each category.\n",
        "\n",
        "Most commonly used for nominal data.\n",
        "\n",
        "c.Drop-One (Reference Category) Encoding\n",
        "\n",
        "One category is treated as a baseline to avoid the dummy variable trap.\n",
        "\n",
        "d.Ordinal Encoding\n",
        "\n",
        "Categories are encoded based on a meaningful order (e.g., low, medium, high).\n",
        "\n",
        "e. Binary Encoding\n",
        "\n",
        "Converts categories into binary digits to reduce dimensionality.\n",
        "\n",
        "f. Target Encoding (Mean Encoding)\n",
        "\n",
        "Replaces categories with the mean of the target variable for that category."
      ],
      "metadata": {
        "id": "W-cOpSrOxGyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "In Multiple Linear Regression, interaction terms are used to capture situations where the effect of one independent variable on the dependent variable depends on another independent variable.\n",
        "\n",
        "Role of interaction terms:\n",
        "\n",
        "-They allow the model to represent combined effects of variables, not just individual effects.\n",
        "\n",
        "-They help explain relationships that are not purely additive.\n",
        "\n",
        "-They improve model accuracy when variables influence each other.\n",
        "\n",
        "Example:\n",
        "\n",
        "The effect of study hours on exam score may depend on teaching method. An interaction term between these two variables captures this dependency."
      ],
      "metadata": {
        "id": "pj380PYRyBRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "The interpretation of the intercept differs based on the number of independent variables in the model.\n",
        "\n",
        "In Simple Linear Regression:\n",
        "\n",
        "-The intercept represents the expected value of Y when the single independent variable X is zero.\n",
        "\n",
        "-It often has a clear and direct interpretation.\n",
        "\n",
        "In Multiple Linear Regression:\n",
        "\n",
        "-The intercept represents the expected value of Y when all independent variables are zero at the same time.\n",
        "\n",
        "-This situation may be unrealistic or outside the data range, making the intercept harder to interpret."
      ],
      "metadata": {
        "id": "vOMPaW31yoIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "In regression analysis, the slope (coefficient of an independent variable) is highly significant because it quantifies the effect of that variable on the dependent variable.\n",
        "\n",
        "Significance of the slope:\n",
        "\n",
        "-Direction of relationship\n",
        "\n",
        ">Positive slope → Y increases as X increases\n",
        "\n",
        ">Negative slope → Y decreases as X increases\n",
        "\n",
        "-Magnitude of effect\n",
        "\n",
        ">Shows how much Y changes for a one-unit change in X, holding other variables constant (in multiple regression).\n",
        "\n",
        "-Prediction impact\n",
        "\n",
        ">The slope directly influences predicted values of Y. A higher magnitude of slope means changes in X have a bigger impact on predictions.\n",
        "\n",
        "-Statistical testing\n",
        "\n",
        ">Slopes are tested (t-tests) to check if the variable significantly contributes to explaining Y."
      ],
      "metadata": {
        "id": "ozqOJyM8zCXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept in a regression model provides context by serving as the baseline value of the dependent variable (Y) when all independent variables are zero.\n",
        "\n",
        "How it provides context:\n",
        "\n",
        "a. Reference point\n",
        "\n",
        ">It shows the starting point of Y before any effect from X variables.\n",
        "\n",
        "b. Helps understand relative changes\n",
        "\n",
        ">Changes in Y due to X can be interpreted relative to this baseline.\n",
        "\n",
        "c. Model completeness\n",
        "\n",
        ">Even if X values are far from zero, the intercept ensures the regression line is anchored, giving a meaningful equation for predictions."
      ],
      "metadata": {
        "id": "GHPmWbIzzj5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Using R² as the only measure of model performance has several limitations:\n",
        "\n",
        "a. Doesn’t indicate bias or accuracy\n",
        "\n",
        ">A high R² doesn’t guarantee that the predictions are accurate; the model could still be biased.\n",
        "\n",
        "b. Ignores overfitting\n",
        "\n",
        ">Adding more variables always increases R², even if they don’t meaningfully improve the model.\n",
        "\n",
        "c. Doesn’t show individual variable significance\n",
        "\n",
        ">R² doesn’t tell which predictors are important or statistically significant.\n",
        "\n",
        "d. Not suitable for non-linear relationships\n",
        "\n",
        ">If the relationship between X and Y is non-linear, R² may be misleading.\n",
        "\n",
        "e. Sensitive to outliers\n",
        "\n",
        ">Extreme values can inflate or deflate R², giving a distorted view of model fit."
      ],
      "metadata": {
        "id": "U0kYCHtYz4_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error for a regression coefficient indicates high uncertainty in the estimate of that coefficient.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "a. Less precise estimate\n",
        "\n",
        "-The coefficient might vary widely if we took different samples from the data.\n",
        "\n",
        "b. Reduced statistical significance\n",
        "\n",
        "-A large standard error makes it harder to reject the null hypothesis that the coefficient is zero.\n",
        "\n",
        "-The corresponding t-value becomes smaller, and the p-value becomes larger.\n",
        "\n",
        "c. Potential causes\n",
        "\n",
        "-High variability in the data\n",
        "\n",
        "-Multicollinearity among independent variables\n",
        "\n",
        "-Small sample size"
      ],
      "metadata": {
        "id": "abaJlu3K0Y7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Heteroscedasticity can be identified in residual plots and is important to address because it affects the reliability of your regression model.\n",
        "\n",
        "How to identify it:\n",
        "\n",
        "a. Plot residuals vs. predicted values (or vs. an independent variable).\n",
        "\n",
        "b. Look for patterns in the spread of residuals:\n",
        "\n",
        "-If the residuals fan out or funnel in as predicted values increase, it indicates heteroscedasticity.\n",
        "\n",
        "-Ideally, residuals should be randomly scattered with constant spread (homoscedastic).\n",
        "\n",
        "Example patterns:\n",
        "\n",
        "-Residuals get larger as X increases → “fanning out”\n",
        "\n",
        "-Residuals shrink as X increases → “funneling in”\n",
        "\n",
        "Why it’s important to address:\n",
        "\n",
        "a. Unreliable standard errors\n",
        "\n",
        "Coefficient estimates may still be unbiased, but standard errors are incorrect.\n",
        "\n",
        "b. Invalid statistical tests\n",
        "\n",
        "t-tests and F-tests may give wrong significance results, leading to misleading conclusions.\n",
        "\n",
        "c. Poor confidence intervals\n",
        "\n",
        "Confidence intervals may be too narrow or too wide, affecting prediction reliability."
      ],
      "metadata": {
        "id": "-mZNQxA90zEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "If a Multiple Linear Regression model has a high R² but a low adjusted R², it usually means:\n",
        "\n",
        "a. Many irrelevant predictors\n",
        "\n",
        "-R² always increases when you add more variables, even if they don’t actually improve the model.\n",
        "\n",
        "b. Adjusted R² penalizes unnecessary variables\n",
        "\n",
        "-Adjusted R² decreases if added predictors don’t contribute meaningful explanatory power.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "-The model appears to fit the data well according to R², but in reality, some variables may be useless or redundant.\n",
        "\n",
        "-The low adjusted R² suggests the model may be overfitting."
      ],
      "metadata": {
        "id": "RGByhFR62d0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling variables in Multiple Linear Regression is important because it ensures that all independent variables are on a comparable scale, which affects model performance and interpretation.\n",
        "\n",
        "Reasons to scale variables:\n",
        "\n",
        "a. Improves numerical stability\n",
        "\n",
        "Large differences in variable magnitudes can cause computational issues in coefficient estimation.\n",
        "\n",
        "b. Makes coefficients comparable\n",
        "\n",
        "Without scaling, variables with larger units may appear more important, even if they aren’t.\n",
        "\n",
        "c. Essential for regularization methods\n",
        "\n",
        "Techniques like Ridge and Lasso Regression require scaled variables to apply penalties fairly across coefficients.\n",
        "\n",
        "d. Faster convergence in optimization\n",
        "\n",
        "Algorithms like gradient descent converge faster when features are scaled.\n",
        "\n",
        "Common scaling methods:\n",
        "\n",
        "-Standardization (Z-score) → subtract mean, divide by standard deviation\n",
        "\n",
        "-Min-Max scaling → scale values to a fixed range, usually 0–1"
      ],
      "metadata": {
        "id": "BSMmNWRI28lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "Polynomial Regression:\n",
        "\n",
        "-Polynomial regression is like linear regression, but instead of fitting a straight line, it fits a curve to the data.\n",
        "\n",
        "-It does this by adding powers of the independent variable(s) as new features. For example:\n",
        "\n",
        ">Original feature: X\n",
        "\n",
        ">New features: X², X³, X⁴, etc.\n",
        "\n",
        "-The model learns a coefficient for each feature (X, X², X³…) to make predictions.\n",
        "\n",
        "-Useful for capturing non-linear relationships between X and Y.\n",
        "\n",
        "Example:\n",
        "\n",
        "-Predicting sales based on advertising spend:\n",
        "\n",
        ">At low spending, sales increase quickly.\n",
        "\n",
        ">At high spending, sales increase slowly or level off.\n",
        "\n",
        "-Using X² in the model allows the regression to fit this curve, instead of forcing a straight line."
      ],
      "metadata": {
        "id": "fasiXQ073ZAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "a. Linear Regression:\n",
        "\n",
        "-Models the relationship between the independent variable(s) and the dependent variable as a straight line.\n",
        "\n",
        "-Example: Predicting Y using X directly.\n",
        "\n",
        "-Only uses the original feature(s) as input.\n",
        "\n",
        "-Works well when the relationship between X and Y is roughly linear.\n",
        "\n",
        "b. Polynomial Regression:\n",
        "\n",
        "-Models the relationship as a curve instead of a straight line.\n",
        "\n",
        "-Achieved by adding powers of the original variable(s) as new features: X², X³, X⁴, etc.\n",
        "\n",
        "-Example: Predicting Y using X, X², and X³.\n",
        "\n",
        "-Useful when the relationship between X and Y is non-linear.\n",
        "\n",
        "c. Complexity:\n",
        "\n",
        "-Linear regression is simpler, fewer parameters to estimate.\n",
        "\n",
        "-Polynomial regression is more complex, more parameters to estimate.\n",
        "\n",
        "d. Flexibility:\n",
        "\n",
        "-Linear regression fits a straight line, limited flexibility.\n",
        "\n",
        "-Polynomial regression fits a curve, more flexible to capture patterns.\n",
        "\n",
        "e. Use case:\n",
        "\n",
        "-Linear regression → relationship between X and Y is roughly straight.\n",
        "\n",
        "-Polynomial regression → relationship between X and Y is curved or more complicated."
      ],
      "metadata": {
        "id": "yv5F1BBA4Tnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "Polynomial Regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear.\n",
        "\n",
        "Situations to use Polynomial Regression:\n",
        "\n",
        "a. Curved relationships\n",
        "\n",
        "-When data points form a curve rather than a straight line.\n",
        "\n",
        "-Example: Sales vs. advertising spend (increases rapidly at first, then levels off).\n",
        "\n",
        "b. Non-linear trends in data\n",
        "\n",
        "-When linear regression cannot capture the pattern of the data.\n",
        "\n",
        "c. Better prediction accuracy\n",
        "\n",
        "-When adding polynomial terms (X², X³, etc.) helps the model fit the data more closely.\n",
        "\n",
        "d. Scientific and engineering applications\n",
        "\n",
        "-Modeling growth, decay, or any phenomenon that doesn’t follow a straight line."
      ],
      "metadata": {
        "id": "HWSw6Vtd5G6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "General idea:\n",
        "\n",
        "-Polynomial regression predicts Y using X raised to different powers.\n",
        "\n",
        "-The model includes a constant term (intercept) and coefficients for each power of X.\n",
        "\n",
        "Step-by-step form:\n",
        "\n",
        "a. Start with a constant term (like intercept).\n",
        "\n",
        "b. Add X multiplied by a coefficient (linear term).\n",
        "\n",
        "c. Add X squared multiplied by a coefficient (quadratic term).\n",
        "\n",
        "d. Add X cubed multiplied by a coefficient (cubic term).\n",
        "\n",
        "e. Continue up to X to the nth power, depending on the degree of the polynomial\n",
        "\n",
        "In words:\n",
        "\n",
        ">Y = intercept + (coefficient × X) + (coefficient × X²) + (coefficient × X³) + … + (coefficient × Xⁿ)\n",
        "\n",
        "Example:\n",
        "\n",
        ">Degree 2 polynomial (quadratic):\n",
        "\n",
        "Y = intercept + a1X + a2X²\n",
        "\n",
        ">Degree 3 polynomial (cubic):\n",
        "\n",
        "Y = intercept + a1X + a2X² + a3*X³"
      ],
      "metadata": {
        "id": "QgCUllUG5a0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes! Polynomial Regression can be applied to multiple variables, and it’s called Multivariate Polynomial Regression.\n",
        "\n",
        "How it works:\n",
        "\n",
        "a. Start with multiple independent variables: X₁, X₂, …, Xₙ.\n",
        "\n",
        "b. Include powers of each variable (X₁², X₂², …) to capture non-linear effects.\n",
        "\n",
        "c. Include interaction terms between variables (X₁ × X₂, X₁² × X₂, etc.) to capture combined effects.\n",
        "\n",
        "d. The model learns coefficients for all these terms to predict Y.\n",
        "\n",
        "Example:\n",
        "\n",
        "-Predicting house price (Y) using:\n",
        "\n",
        "X₁ = house size, X₂ = number of rooms\n",
        "\n",
        "-Polynomial terms:\n",
        "\n",
        "X₁², X₂², X₁×X₂\n",
        "\n",
        "-Model: Y = intercept + a₁X₁ + a₂X₂ + a₃X₁² + a₄X₂² + a₅*(X₁×X₂)"
      ],
      "metadata": {
        "id": "7XQQtvM758wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "a. Overfitting\n",
        "\n",
        "Higher-degree polynomials can fit the training data too closely, capturing noise instead of the true pattern.\n",
        "\n",
        "This makes the model perform poorly on new data.\n",
        "\n",
        "b. Poor extrapolation\n",
        "\n",
        "Polynomial models can behave unpredictably outside the range of the training data.\n",
        "\n",
        "c. Increased complexity\n",
        "\n",
        "Adding higher powers or multiple variables increases the number of terms, making the model harder to interpret.\n",
        "\n",
        "d. Sensitive to outliers\n",
        "\n",
        "Outliers can drastically affect the curve because polynomial terms amplify deviations.\n",
        "\n",
        "e. Multicollinearity\n",
        "\n",
        "Powers of X (X², X³, etc.) are often highly correlated with each other, which can make coefficient estimates unstable.\n",
        "\n",
        "f. Not always better than linear models\n",
        "\n",
        "For some datasets, simpler models or other non-linear models (like decision trees or splines) may work better and more reliably."
      ],
      "metadata": {
        "id": "8vM5Cd5s6hYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "When selecting the degree of a polynomial in polynomial regression, you want to choose a degree that fits the data well without overfitting. Several methods can be used to evaluate model fit:\n",
        "\n",
        "a. Visual inspection\n",
        "\n",
        "-Plot the polynomial curve against the data.\n",
        "\n",
        "-Check if the curve captures the trend without being too wiggly (overfitting) or too flat (underfitting).\n",
        "\n",
        "b. R² (Coefficient of determination)\n",
        "\n",
        "-Measures how much variance in Y is explained by the model.\n",
        "\n",
        "-Higher R² indicates better fit, but be careful: R² always increases with higher degree, even if unnecessary.\n",
        "\n",
        "c. Adjusted R²\n",
        "\n",
        "-Adjusts R² for the number of predictors.\n",
        "\n",
        "-Helps penalize unnecessary terms, making it better for selecting polynomial degree.\n",
        "\n",
        "d. Cross-Validation\n",
        "\n",
        "-Split the data into training and validation sets, or use k-fold cross-validation.\n",
        "\n",
        "-Evaluate prediction error (like RMSE or MAE) on validation data.\n",
        "\n",
        "-Helps prevent overfitting when selecting polynomial degree.\n",
        "\n",
        "e. RMSE / MAE (Root Mean Squared Error / Mean Absolute Error)\n",
        "\n",
        "-Compute error between predicted and actual values.\n",
        "\n",
        "-Lower error indicates better fit. Can be used on training or validation data.\n",
        "\n",
        "f. Information Criteria\n",
        "\n",
        "-AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) penalize model complexity.\n",
        "\n",
        "-Lower values indicate a better balance of fit and simplicity."
      ],
      "metadata": {
        "id": "cRw7H93b7g6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization is very important in polynomial regression because it helps you understand the relationship between variables and assess the model fit.\n",
        "\n",
        "Reasons why visualization is important:\n",
        "\n",
        "a. See the curve\n",
        "\n",
        "-Polynomial regression fits a curve, not a straight line.\n",
        "\n",
        "-Plotting the model shows whether the curve matches the trend of the data.\n",
        "\n",
        "b. Detect underfitting or overfitting\n",
        "\n",
        "-If the curve is too flat → underfitting (too low degree).\n",
        "\n",
        "-If the curve is too wiggly → overfitting (too high degree).\n",
        "\n",
        "c. Check residuals\n",
        "\n",
        "-Plotting residuals vs predicted values helps identify heteroscedasticity, non-linear patterns, or outliers.\n",
        "\n",
        "d. Communicate results clearly\n",
        "\n",
        "-Graphs make it easier for others to see how X affects Y, especially for complex, non-linear relationships.\n",
        "\n",
        "e. Choose the right degree\n",
        "\n",
        "-Comparing curves of different polynomial degrees visually helps pick the simplest degree that fits the data well."
      ],
      "metadata": {
        "id": "8g_ors5p8qDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "a. Import libraries\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "b. Prepare the data\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
        "\n",
        "Y = np.array([1, 4, 9, 16, 25, 36, 49, 64, 81])  # roughly Y = X^2\n",
        "\n",
        "c. Transform features to polynomial\n",
        "\n",
        "degree = 2  # Degree of the polynomial\n",
        "\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "-This creates new features: X, X² (and intercept automatically).\n",
        "\n",
        "d. Fit linear regression on polynomial features\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "e. Evaluate the model\n",
        "\n",
        "print(\"R²:\", r2_score(Y, Y_pred))\n",
        "\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(Y, Y_pred)))\n",
        "\n",
        "f. Visualize the fit\n",
        "\n",
        "plt.scatter(X, Y, color='blue', label='Actual')\n",
        "\n",
        "plt.plot(X, Y_pred, color='red', label='Polynomial fit')\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "\n",
        "plt.ylabel(\"Y\")\n",
        "\n",
        "plt.title(f\"Polynomial Regression (degree={degree})\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P5tmKfOO9LYx"
      }
    }
  ]
}